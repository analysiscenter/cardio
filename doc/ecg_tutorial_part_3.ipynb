{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 3 we learn how to train and predict built-in ecg models. We consider [fft_inceprion](https://github.com/analysiscenter/ecg/blob/unify_models/doc/fft_model.md) model as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary imports before to start. Note ```ModelEcgBatch``` that contains models is imported rather than plain ```EcgBatch```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import ecg.dataset as ds\n",
    "from ecg.batch import ModelEcgBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an ecg dataset (see [Notebook 1](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_1.ipynb) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ds.FilesIndex(path=\".../data/ECG/*.hea\", no_ext=True, sort=True)\n",
    "eds = ds.Dataset(index, batch_class=ModelEcgBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to divide the dataset into 2 parts that will be used for train and validation. Method ```cv_splint``` do this job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eds.cv_split(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 80% of the dataset are in ```eds.train``` and the rest in ```eds.test```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a preprocess pipeline. Ttis part is common for train and prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = (ds.Pipeline()\n",
    "                       .load(fmt=\"wfdb\", components=[\"signal\", \"meta\"])\n",
    "                       .load(src=\".../data/ECG/REFERENCE.csv\",\n",
    "                             fmt=\"csv\", components=\"target\")\n",
    "                       .drop_labels([\"~\"])\n",
    "                       .replace_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "                       .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "                       .drop_short_signals(4000)\n",
    "                       .segment_signals(3000, 3000)\n",
    "                       .binarize_labels()\n",
    "                       .apply(np.transpose, [0, 2, 1])\n",
    "                       .ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train pipeline is preprocess pipeline plus ```train_on_batch``` action. We exploit pipeline algebra to merge two pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_train_pipeline = (preprocess_pipeline +\n",
    "                      ds.Pipeline().train_on_batch('fft_inception', metrics=f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we only have to pass dataset to pipeline and start the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fft_trained = (eds.train >> fft_train_pipeline).run(batch_size=300, shuffle=True, drop_last=True, n_epochs=1, prefetch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we obtain pipeline ```fft_trained``` that contains trained model. Let's make a prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict pipeline is preprocess pipeline plus ```import_model``` action plus ```predict_on_batch``` action. Model can be imported from dump file or from pipeline with trained model. We show the second option since we have ```fft_trained``` pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fft_predict_pipeline = ((ds.Pipeline()\n",
    "                         .import_model('fft_inception', fft_trained)\n",
    "                         .init_variable(\"prediction\", [])) +\n",
    "                        preprocess_pipeline +\n",
    "                        ds.Pipeline().predict_on_batch('fft_inception'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we aslo add action ```init_variable```. It defines empty list ```prediction``` that will store output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start caclulation we pass ecg dataset into pipeline and call action ```run```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = (eds.test >> fft_predict_pipeline).run(batch_size=300, shuffle=False, drop_last=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output we read pipeline variable ```prediction```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.03159175,  0.96840829],\n",
      "       [ 0.05374619,  0.94625378],\n",
      "       [ 0.05049713,  0.94950283],\n",
      "       ..., \n",
      "       [ 0.0613869 ,  0.93861312],\n",
      "       [ 0.06168694,  0.93831307],\n",
      "       [ 0.06264812,  0.93735182]], dtype=float32), array([[ 0.02915464,  0.9708454 ],\n",
      "       [ 0.02819072,  0.97180927],\n",
      "       [ 0.02667561,  0.97332442],\n",
      "       ..., \n",
      "       [ 0.02050101,  0.97949904],\n",
      "       [ 0.04027931,  0.95972073],\n",
      "       [ 0.03398024,  0.96601969]], dtype=float32), array([[ 0.00240983,  0.99759018],\n",
      "       [ 0.02517843,  0.97482163],\n",
      "       [ 0.04068676,  0.95931321],\n",
      "       ..., \n",
      "       [ 0.0757554 ,  0.92424452],\n",
      "       [ 0.08056658,  0.91943341],\n",
      "       [ 0.07819081,  0.92180914]], dtype=float32), array([[ 0.01943815,  0.98056191],\n",
      "       [ 0.06241378,  0.93758619],\n",
      "       [ 0.062431  ,  0.93756902],\n",
      "       ..., \n",
      "       [ 0.04630136,  0.95369864],\n",
      "       [ 0.0429494 ,  0.95705062],\n",
      "       [ 0.04455574,  0.95544428]], dtype=float32), array([[ 0.06281535,  0.93718469],\n",
      "       [ 0.07309279,  0.92690724],\n",
      "       [ 0.06642715,  0.93357289],\n",
      "       ..., \n",
      "       [ 0.02539047,  0.97460961],\n",
      "       [ 0.03097754,  0.96902239],\n",
      "       [ 0.03972384,  0.96027619]], dtype=float32), array([[ 0.0281729 ,  0.97182703],\n",
      "       [ 0.05454158,  0.94545841],\n",
      "       [ 0.04900014,  0.95099986],\n",
      "       ..., \n",
      "       [ 0.04149741,  0.95850265],\n",
      "       [ 0.03818701,  0.96181303],\n",
      "       [ 0.03627867,  0.96372128]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predicted.get_variable('prediction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Notebook 3. See previous topics in [Notebook 1](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_1.ipynb) and [Notebook 2](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_2.ipynb). See more on ecg models [here](https://github.com/analysiscenter/ecg/blob/unify_models/doc/models.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
